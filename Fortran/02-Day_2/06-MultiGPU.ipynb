{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb68e92d",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Multi GPU programming with OpenACC\n",
    "\n",
    "---\n",
    "**Requirements:**\n",
    "\n",
    "- [Get started](./Get_started.ipynb)\n",
    "- [Data Management](./Data_management.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3428df",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "This part requires that you have a basic knowledge of OpenMP and/or MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab0614",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "If you wish to have your code run on multiple GPUs, several strategies are available. The most simple ones are to create either several threads or MPI tasks, each one addressing one GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dc90d",
   "metadata": {
    "editable": false
   },
   "source": [
    "## API description\n",
    "\n",
    "For this part, the following API functions are needed:\n",
    "\n",
    "- *acc_get_device_type()*: retrieve the type of accelerator available on the host\n",
    "- *acc_get_num_device(device_type)*: retrieve the number of accelerators of the given type\n",
    "- *acc_set_device_num(id, device_type)*: set the id of the device of the given type to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa549cd",
   "metadata": {
    "editable": false
   },
   "source": [
    "## MPI strategy\n",
    "\n",
    "In this strategy, you will follow a classical MPI procedure where several tasks are executed. We will use either the OpenACC directive or API to make each task use 1 GPU.\n",
    "\n",
    "Have a look at the [examples/C/init_openacc.h](../../examples/C/init_openacc.h)\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/Fortran/MultiGPU_mpi_example.f90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun -m 4 -a --gpus 2 --option \"-cpp\"\n",
    "! you should add ` --option \"-cpp\" ` as argument to the idrrun command\n",
    "program multigpu\n",
    "    use ISO_FORTRAN_ENV, only : INT32\n",
    "    use mpi\n",
    "    use openacc\n",
    "    implicit none\n",
    "    integer(kind=INT32), dimension(100) :: a\n",
    "    integer                             :: comm_size, my_rank, code, i\n",
    "    integer                             :: num_gpus, my_gpu\n",
    "    integer(kind=acc_device_kind)       :: device_type\n",
    "\n",
    "    ! Useful for OpenMPI and GPU DIRECT\n",
    "    call initialisation_openacc()\n",
    "\n",
    "    ! MPI stuff\n",
    "    call MPI_Init(code)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, comm_size, code)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, code)\n",
    "\n",
    "    ! OpenACC stuff\n",
    "    #ifdef _OPENACC\n",
    "    device_type = acc_get_device_type()\n",
    "    num_gpus = acc_get_num_devices(device_type)\n",
    "    my_gpu   = mod(my_rank,num_gpus)\n",
    "    call acc_set_device_num(my_gpu, device_type)\n",
    "    my_gpu   = acc_get_device_num(device_type)   \n",
    "    ! Alternatively you can set the GPU number with #pragma acc set device_num(my_gpu)\n",
    "\n",
    "    !$acc parallel loop\n",
    "    do i = 1, 100\n",
    "        a(i) = i\n",
    "    enddo   \n",
    "    #endif\n",
    "    write(0,\"(a13,i2,a17,i2,a8,i2,a10,i2)\") \"Here is rank \",my_rank,\": I am using GPU \",my_gpu, & \n",
    "                                            \" of type \",device_type,\". a(42) = \",a(42)\n",
    "    call MPI_Finalize(code)\n",
    "\n",
    "    contains\n",
    "        #ifdef _OPENACC\n",
    "        subroutine initialisation_openacc\n",
    "        use openacc\n",
    "        \n",
    "        type accel_info\n",
    "            integer :: current_devices\n",
    "            integer :: total_devices\n",
    "        end type accel_info\n",
    "       \n",
    "        type(accel_info) :: info\n",
    "        character(len=6) :: local_rank_env\n",
    "        integer          :: local_rank_env_status, local_rank\n",
    "        ! Initialisation of OpenACC\n",
    "        !$acc init\n",
    " \n",
    "       ! Recovery of the local rank of the process via the environment variable\n",
    "       ! set by Slurm, as MPI_Comm_rank cannot be used here because this routine\n",
    "       ! is used BEFORE the initialisation of MPI\n",
    "       call get_environment_variable(name=\"SLURM_LOCALID\", value=local_rank_env, status=local_rank_env_status)\n",
    "       info%total_devices = acc_get_num_devices(acc_get_device_type())\n",
    "       if (local_rank_env_status == 0) then\n",
    "           read(local_rank_env, *) local_rank\n",
    "           ! Definition of the GPU to be used via OpenACC\n",
    "           call acc_set_device_num(local_rank, acc_get_device_type())\n",
    "           info%current_devices = local_rank\n",
    "       else\n",
    "           print *, \"Error : impossible to determine the local rank of the process\"\n",
    "           stop 1\n",
    "       endif\n",
    "       end subroutine initialisation_openacc\n",
    "       #endif    \n",
    "\n",
    "end program multigpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d172424",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Remarks\n",
    "\n",
    "It is possible to have several tasks accessing the same GPU. It can be useful if one task is not enough to keep the GPU busy along the computation.\n",
    "\n",
    "If you use NVIDIA GPU, you should have a look at the [Multi Process Service](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5054510",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Multithreading strategy\n",
    "\n",
    "Another way to use several GPUs is with multiple threads. Each thread will use one GPU and several threads can share 1 GPU.\n",
    "\n",
    "Example stored in: `../../examples/Fortran/MultiGPU_openmp_example.f90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c76c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun -a -t -g 4 --threads 4 --option \"-cpp\"\n",
    "! you should add ` --option \"-cpp\" ` as argument to the idrrun command\n",
    "program MultiGPU_openmp\n",
    "    use ISO_FORTRAN_ENV, only : INT32\n",
    "    use OMP_LIB\n",
    "    use openacc\n",
    "    implicit none    \n",
    "    integer(kind=INT32)           :: my_rank\n",
    "    integer                       :: num_gpus, my_gpu\n",
    "    integer(kind=acc_device_kind) :: device_type\n",
    "\n",
    "    !$omp parallel private(my_rank, my_gpu, device_type)\n",
    "        my_rank = omp_get_thread_num()\n",
    "        ! OpenACC Stuff\n",
    "        #ifdef _OPENACC\n",
    "        device_type = acc_get_device_type()\n",
    "        num_gpus = acc_get_num_devices(device_type)\n",
    "        my_gpu   = mod(my_rank,num_gpus)\n",
    "        call acc_set_device_num(my_gpu, device_type)\n",
    "        ! We check what GPU is really in use\n",
    "        my_gpu = acc_get_device_num(device_type)\n",
    "        ! Alternatively you can set the GPU number with #pragma acc set device_num(my_gpu)\n",
    "        write(0,\"(a14,i2,a17,i2,a9,i2)\") \"Here is thread \",my_rank,\" : I am using GPU \",my_gpu,\" of type \",device_type\n",
    "        #endif  \n",
    "    !$omp end parallel\n",
    "end program MultiGPU_openmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a4855",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Copy one cell from a previous notebook with a sequential code\n",
    "2. Modify the code to use several GPUs\n",
    "3. Check the correctness of the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1dd55",
   "metadata": {
    "editable": false
   },
   "source": [
    "## GPU to GPU data transfers\n",
    "\n",
    "If you have several GPUs on your machine they are likely interconnected. For NVIDIA GPUs, there are 2 flavors of connections: either PCI express or NVlink.\n",
    "[NVLink](https://www.nvidia.com/fr-fr/data-center/nvlink/) is a fast interconnect between GPUs. Be careful since it might not be available on your machine.\n",
    "The main difference between the two connections is the bandwidth for CPU/GPU transfers, which is higher for NVlink.\n",
    "\n",
    "The GPUDirect feature of CUDA-aware MPI libraries allows direct data transfers between GPUs without an intermediate copy to the CPU memory. If you have access to an MPI CUDA-aware implementation with GPUDirect support, you should definitely adapt your code to benefit from this feature.\n",
    "\n",
    "For information, during this training course we are using OpenMPI which is CUDA-aware.\n",
    "You can find a list of CUDA-aware implementation on [NVIDIA website](https://developer.nvidia.com/mpi-solutions-gpus).\n",
    "\n",
    "By default, the data transfers between GPUs are not direct. The scheme is the following:\n",
    "\n",
    "1. The __origin__ task generates a Device to Host data transfer\n",
    "2. The __origin__ task sends the data to the __destination__ task.\n",
    "3. The __destination__ task generates a Host to Device data transfer\n",
    "\n",
    "Here we can see that 2 transfers between Host and Device are necessary. This is costly and should be avoided if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436750c",
   "metadata": {
    "editable": false
   },
   "source": [
    "### `acc host_data` directive\n",
    "\n",
    "To be able to transfer data directly between GPUs, we introduce the __host_data__ directive.\n",
    "```fortran\n",
    "!$acc host_data use_device(array)\n",
    "...\n",
    "!$acc end host_data\n",
    "```\n",
    "\n",
    "\n",
    "This directive tells the compiler to assign the address of the variable to its value on the device.\n",
    "You can then use the pointer with your MPI calls.\n",
    "__You have to call the MPI functions on the host.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707fc43",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here is a example of a code using GPU to GPU direct transfer.\n",
    "```fortran\n",
    "integer, parameter :: system_size = 1000;\n",
    "integer, dimension(system_size) :: array\n",
    "!$acc enter_data create(array(1:1000))\n",
    "! Perform some stuff on the GPU\n",
    "!$acc parallel present(array(1:1000))\n",
    "...\n",
    "!$acc end parallel\n",
    "! Transfer the data between GPUs\n",
    "if (my_rank .eq. origin ) then\n",
    "    !$acc host_data use_device(array)\n",
    "    MPI_Send(array, size, MPI_INT, destination, tag, MPI_COMM_WORLD, code)\n",
    "    !$acc end host_data\n",
    "endif\n",
    "if (my_rank .eq. destination) then\n",
    "    !$acc host_data use_device(array)\n",
    "    MPI_Recv(array, size, MPI_INT, origin, tag, MPI_COMM_WORLD, status, code)\n",
    "    !$acc end host_data\n",
    "endif\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f527d",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "As an exercise, you can complete the following MPI code that measures the bandwidth between the GPUs:\n",
    "\n",
    "1. Add directives to create the buffers on the GPU\n",
    "2. Measure the effective bandwidth between GPUs by adding the directives necessary to transfer data from one GPU to another one in the following cases:\n",
    "\n",
    "- Not using NVLink\n",
    "- Using NVLink\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/Fortran/MultiGPU_mpi_exercise.f90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun -m 4 -a --gpus 2 --option \"-cpp\"\n",
    "! you should add ` --option \"-cpp\" ` as argument to the idrrun command\n",
    "program MultiGPU_exercice\n",
    "    use ISO_FORTRAN_ENV, only : INT32, REAL64\n",
    "    use mpi\n",
    "    use openacc\n",
    "    implicit none\n",
    "    real   (kind=REAL64), dimension(:), allocatable :: send_buffer, receive_buffer\n",
    "    real   (kind=REAL64)                            :: start, finish , data_volume   \n",
    "    integer(kind=INT32 ), parameter                 :: system_size = 2e8/8\n",
    "    integer                                         :: comm_size, my_rank, code, reps, i, j, k\n",
    "    integer                                         :: num_gpus, my_gpu\n",
    "    integer(kind=acc_device_kind)                   :: device_type\n",
    "    integer, dimension(MPI_STATUS_SIZE)             :: mpi_stat\n",
    "\n",
    "    ! Useful for OpenMPI and GPU DIRECT\n",
    "    call initialisation_openacc()\n",
    "\n",
    "    ! MPI stuff\n",
    "    reps = 5\n",
    "    data_volume = dble(reps*system_size)*8*1024_real64**(-3.0)\n",
    "\n",
    "    call MPI_Init(code)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, comm_size, code)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, code)\n",
    "    allocate(send_buffer(system_size), receive_buffer(system_size))\n",
    "\n",
    "    ! OpenACC stuff\n",
    "    #ifdef _OPENACC\n",
    "    device_type = acc_get_device_type()\n",
    "    num_gpus = acc_get_num_devices(device_type)\n",
    "    my_gpu   = mod(my_rank,num_gpus)\n",
    "    call acc_set_device_num(my_gpu, device_type)\n",
    "    #endif\n",
    "\n",
    "    do j = 0, comm_size - 1\n",
    "        do i = 0, comm_size - 1\n",
    "            if ( (my_rank .eq. j) .and. (j .ne. i) ) then\n",
    "                start = MPI_Wtime()\n",
    "                do k = 1, reps\n",
    "                    call MPI_Send(send_buffer,system_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, code)\n",
    "                enddo\n",
    "            endif \n",
    "            if ( (my_rank .eq. i) .and. (i .ne. j) ) then\n",
    "                do k = 1, reps\n",
    "                    call MPI_Recv(receive_buffer, system_size, MPI_DOUBLE, j, 0, MPI_COMM_WORLD, mpi_stat, code)\n",
    "                enddo\n",
    "            endif\n",
    "            if ( (my_rank .eq. j) .and. (j .ne. i) ) then\n",
    "                finish = MPI_Wtime()\n",
    "                write(0,\"(a11,i2,a2,i2,a2,f20.8,a5)\") \"bandwidth \",j,\"->\",i,\": \",data_volume/(finish-start),\" GB/s\"\n",
    "            endif\n",
    "        enddo\n",
    "    enddo\n",
    "    \n",
    "    deallocate(send_buffer, receive_buffer)\n",
    "    \n",
    "    call MPI_Finalize(code)\n",
    "\n",
    "    contains\n",
    "        #ifdef _OPENACC\n",
    "        subroutine initialisation_openacc\n",
    "            use openacc\n",
    "            implicit none\n",
    "            type accel_info\n",
    "                integer :: current_devices\n",
    "                integer :: total_devices\n",
    "            end type accel_info\n",
    "\n",
    "            type(accel_info) :: info\n",
    "            character(len=6) :: local_rank_env\n",
    "            integer          :: local_rank_env_status, local_rank\n",
    "        ! Initialisation of OpenACC\n",
    "            !$acc init\n",
    "\n",
    "        ! Recovery of the local rank of the process via the environment variable\n",
    "        ! set by Slurm, as MPI_Comm_rank cannot be used here because this routine\n",
    "        ! is used BEFORE the initialisation of MPI\n",
    "            call get_environment_variable(name=\"SLURM_LOCALID\", value=local_rank_env, status=local_rank_env_status)\n",
    "            info%total_devices = acc_get_num_devices(acc_get_device_type())\n",
    "            if (local_rank_env_status == 0) then\n",
    "                read(local_rank_env, *) local_rank\n",
    "                ! Definition of the GPU to be used via OpenACC\n",
    "                call acc_set_device_num(local_rank, acc_get_device_type())\n",
    "                info%current_devices = local_rank\n",
    "            else\n",
    "                print *, \"Error : impossible to determine the local rank of the process\"\n",
    "                stop 1\n",
    "            endif\n",
    "        end subroutine initialisation_openacc\n",
    "        #endif \n",
    "\n",
    "end program MultiGPU_exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c4da0",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### Solution\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/Fortran/MultiGPU_mpi_solution.f90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb34ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%idrrun -m 4 -a --gpus 2 --option \"-cpp\"\n",
    "! you should add ` --option \"-cpp\" ` as argument to the idrrun command\n",
    "program MultiGPU_solution\n",
    "    use ISO_FORTRAN_ENV, only : INT32, REAL64\n",
    "    use mpi\n",
    "    use openacc\n",
    "    implicit none\n",
    "    real   (kind=REAL64), dimension(:), allocatable :: send_buffer, receive_buffer\n",
    "    real   (kind=REAL64)                            :: start, finish , data_volume   \n",
    "    integer(kind=INT32 ), parameter                 :: system_size = 2e8/8\n",
    "    integer                                         :: comm_size, my_rank, code, reps, i, j, k\n",
    "    integer                                         :: num_gpus, my_gpu\n",
    "    integer(kind=acc_device_kind)                   :: device_type\n",
    "    integer, dimension(MPI_STATUS_SIZE)             :: mpi_stat\n",
    "\n",
    "    ! Useful for OpenMPI and GPU DIRECT\n",
    "    call initialisation_openacc()\n",
    "\n",
    "    ! MPI stuff\n",
    "    reps = 5\n",
    "    data_volume = dble(reps*system_size)*8*1024_real64**(-3.0)\n",
    "\n",
    "    call MPI_Init(code)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, comm_size, code)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, code)\n",
    "    allocate(send_buffer(system_size), receive_buffer(system_size))\n",
    "    !$acc enter data create(send_buffer(1:system_size), receive_buffer(1:system_size))\n",
    "\n",
    "    ! OpenACC stuff\n",
    "    #ifdef _OPENACC\n",
    "    device_type = acc_get_device_type()\n",
    "    num_gpus = acc_get_num_devices(device_type)\n",
    "    my_gpu   = mod(my_rank,num_gpus)\n",
    "    call acc_set_device_num(my_gpu, device_type)\n",
    "    #endif\n",
    "\n",
    "    do j = 0, comm_size - 1\n",
    "        do i = 0, comm_size - 1\n",
    "            if ( (my_rank .eq. j) .and. (j .ne. i) ) then\n",
    "                start = MPI_Wtime()\n",
    "                !$acc host_data use_device(send_buffer)\n",
    "                do k = 1, reps\n",
    "                    call MPI_Send(send_buffer,system_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, code)\n",
    "                enddo\n",
    "                !$acc end host_data\n",
    "            endif \n",
    "            if ( (my_rank .eq. i) .and. (i .ne. j) ) then\n",
    "                !$acc host_data use_device(receive_buffer)\n",
    "                do k = 1, reps\n",
    "                    call MPI_Recv(receive_buffer, system_size, MPI_DOUBLE, j, 0, MPI_COMM_WORLD, mpi_stat, code)\n",
    "                enddo\n",
    "                !$acc end host_data\n",
    "            endif\n",
    "            if ( (my_rank .eq. j) .and. (j .ne. i) ) then\n",
    "                finish = MPI_Wtime()\n",
    "                write(0,\"(a11,i2,a2,i2,a2,f20.8,a5)\") \"bandwidth \",j,\"->\",i,\": \",data_volume/(finish-start),\" GB/s\"\n",
    "            endif\n",
    "        enddo\n",
    "    enddo\n",
    "    !$acc exit data delete(send_buffer, receive_buffer)\n",
    "    deallocate(send_buffer, receive_buffer)\n",
    "    \n",
    "    call MPI_Finalize(code)\n",
    "\n",
    "    contains\n",
    "        #ifdef _OPENACC\n",
    "        subroutine initialisation_openacc\n",
    "            use openacc\n",
    "            implicit none\n",
    "            type accel_info\n",
    "                integer :: current_devices\n",
    "                integer :: total_devices\n",
    "            end type accel_info\n",
    "\n",
    "            type(accel_info) :: info\n",
    "            character(len=6) :: local_rank_env\n",
    "            integer          :: local_rank_env_status, local_rank\n",
    "        ! Initialisation of OpenACC\n",
    "            !$acc init\n",
    "\n",
    "        ! Recovery of the local rank of the process via the environment variable\n",
    "        ! set by Slurm, as MPI_Comm_rank cannot be used here because this routine\n",
    "        ! is used BEFORE the initialisation of MPI\n",
    "            call get_environment_variable(name=\"SLURM_LOCALID\", value=local_rank_env, status=local_rank_env_status)\n",
    "            info%total_devices = acc_get_num_devices(acc_get_device_type())\n",
    "            if (local_rank_env_status == 0) then\n",
    "                read(local_rank_env, *) local_rank\n",
    "                ! Definition of the GPU to be used via OpenACC\n",
    "                call acc_set_device_num(local_rank, acc_get_device_type())\n",
    "                info%current_devices = local_rank\n",
    "            else\n",
    "                print *, \"Error : impossible to determine the local rank of the process\"\n",
    "                stop 1\n",
    "            endif\n",
    "        end subroutine initialisation_openacc\n",
    "        #endif \n",
    "\n",
    "end program MultiGPU_solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU Directives",
   "language": "python",
   "name": "gpu_directives"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
