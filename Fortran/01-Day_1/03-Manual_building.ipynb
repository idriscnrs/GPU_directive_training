{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690cd7d4",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Manual building of an OpenACC code\n",
    "\n",
    "---\n",
    "**Requirements:**\n",
    "\n",
    "- [Get started](./Get_started.ipynb)\n",
    "- [Data Management](./Data_management.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "During the training course, the building of examples will be done just by executing the code cells.\n",
    "Even though the command line is always printed, we think it is important to practice the building process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca3615",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Build with NVIDIA compilers\n",
    "\n",
    "The compilers are:\n",
    "\n",
    "- nvc: C compiler\n",
    "- nvc++: C++ compiler\n",
    "- nvfortran: Fortran compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb3f1e",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Compiler options for OpenACC\n",
    "\n",
    "- `-acc`: the compiler will recognize the OpenACC directives\n",
    "\n",
    "    OpenACC is also able to generate code for multicore CPUs (close to OpenMP).\n",
    "\n",
    "    Some interesting options are:\n",
    "  - `-acc=gpu`: to build for GPU\n",
    "  - `-acc=multicore`: to build for CPU (multithreaded)\n",
    "  - `-acc=host`: to build for CPU (sequential)\n",
    "  - `-acc=noautopar`: disable the automatic parallelization inside `parallel` regions (the default is `-acc=autopar`)\n",
    "\n",
    "All options can be found in the [documentation.](https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html#acc-cmdln-opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c88e15",
   "metadata": {
    "editable": false
   },
   "source": [
    "- `-gpu`: GPU-specific options to be passed to the compiler\n",
    "\n",
    "    Some interesting options are:\n",
    "\n",
    "  - `-gpu=ccXX`: specify the compute capability for which the code has to be built\n",
    "\n",
    "      The list is available at [https://developer.nvidia.com/cuda-gpus#compute](https://developer.nvidia.com/cuda-gpus#compute).\n",
    "  - `-gpu=managed`: activate NVIDIA Unified Memory (with it you can ignore data transfers, but it might fail sometime)\n",
    "  - `-gpu=pinned`: activate _pinned_ memory. It can help to improve the performance of data transfers\n",
    "  - `-lineinfo`: generate debugging line information; less overhead than -g\n",
    "\n",
    "All options can be found in the [documentation.](https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html#gpu).\n",
    "\n",
    "- `-Minfo`: the compiler prints information about the optimizations it uses\n",
    "  - `-Minfo=accel`: information about OpenACC (Mandatory in this training course!)\n",
    "  - `-Minfo=all`: all optimizations are printed (OpenACC, vectorization, FMA, ...). We recommend to use this option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb227946",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Other useful compiler options\n",
    "\n",
    "- `-o exec_name`: name of the executable\n",
    "- `-Ox`: level of optimization (0 <= x <=4)\n",
    "- `-Og`: optimize debugging experience and enables optimizations that do not interfere with debugging.\n",
    "- `-fast`: equivalent to `-O2 -Munroll=c:1 -Mnoframei -Mlre`\n",
    "- `-g`: add debugging symbols\n",
    "- `-gopt`: instructs the compiler to include symbolic debugging information in the object file, and to generate optimized code identical to that generated when -g is not specified.\n",
    "\n",
    "You can specify a comma-separated list of options for each flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1064f5",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Examples\n",
    "\n",
    "For instance to compile a C source code for GPU on NVIDIA V100 (Compute Capability 7.0), the following line should be executed:\n",
    "```bash\n",
    "nvfortran -acc=gpu,noautopar -gpu=cc70,managed -Minfo=all mysource.f90 -o myprog\n",
    "```\n",
    "\n",
    "The example below shows how to compile for the following setup:\n",
    "\n",
    "- OpenACC for GPU `-acc=gpu`\n",
    "- Compile for Volta architecture `-gpu=cc70`\n",
    "- Activate optimizations `-fast`\n",
    "- Print optimizations and OpenACC information `-Minfo=all`\n",
    "\n",
    "```make\n",
    "ACCFLAGS = -acc=gpu -gpu=cc70\n",
    "OPTFLAGS = -fast\n",
    "INFOFLAGS = -Minfo=all\n",
    "\n",
    "myacc_exec: myacc.f90\n",
    "    nvfortran -o myacc_exec $(ACCFLAGS) $(OPTFLAGS) $(INFOFLAGS) myacc.f90\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0277edb",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Build with GCC compilers\n",
    "\n",
    "The compilers are:\n",
    "\n",
    "- gcc: C compiler\n",
    "- gxx: C++ compiler\n",
    "- gfortran: Fortran compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b58b5",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Compiler options for OpenACC\n",
    "\n",
    "- `-fopenacc`: the compiler will recognize the OpenACC directives\n",
    "- `-foffload`: enables the compiler to generate a code for the accelerator. Compilers for host and accelerator are separated\n",
    "  - `-foffload=nvptx-none`: compile for NVIDIA devices\n",
    "\n",
    "    It can be used to pass options such as optimization, libraries to link, etc (`-foffload=-O3 -foffload=-lm`).\n",
    "    You can enclose options between \"\" and give it to `-foffload`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1b2fc",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Other useful compiler options\n",
    "\n",
    "- `-o exec_name`: name of the executable\n",
    "- `-Ox`: level of optimization (0 <= x <=3)\n",
    "- `-g`: add debugging symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be4d09",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Example\n",
    "\n",
    "The example shows how to compile for the following setup:\n",
    "\n",
    "- OpenACC for GPU `-fopenacc`\n",
    "- Compile for NVIDIA GPU `-foffload=nvptx-none`\n",
    "- Activate optimizations `-O3 -foffload=-O3`\n",
    "\n",
    "```make\n",
    "ACCFLAGS = -fopenacc -foffload=nvptx-none\n",
    "OPTFLAGS = -O3 -foffload=-O3\n",
    "INFOFLAGS = -fopt-info\n",
    "\n",
    "myacc_exec: myacc.f90\n",
    "    gfortran -o myacc_exec $(ACCFLAGS) $(OPTFLAGS) $(INFOFLAGS) myacc.f90\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120cc39",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "- Execute the following cell which produces a file (just add the name you want after `writefile`).\n",
    "- Open a terminal (File -> New -> Terminal)\n",
    "- Load the compiler you wish to use (for example: `module load nvidia-compiler/21.7`)\n",
    "- Use the information above to compile the file, you might need to modify the extension of the file \"exercise\" to \"exercise.c\" or \"exercise.f90\"\n",
    "- If you want to make sure that the code ran on GPU you can do `export NVCOMPILER_ACC_TIME=1`\n",
    "- Execute the code with `srun -n 1 --cpus-per-task=10 -A for@v100 --gres=gpu:1 --time=00:03:00 --hint=nomultithread --qos=qos_gpu-dev time <executable_name>`\n",
    "- Bonus: Compile the code without OpenACC support and compare the elapsed time in both cases.\n",
    "\n",
    "Example stored in: `../../examples/Fortran/Manual_building_exercise.f90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e515a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercise\n",
    "program manual_build\n",
    "    use iso_fortran_env, only : INT32, REAL64\n",
    "    implicit none\n",
    "\n",
    "    real   (kind=REAL64), dimension(:), allocatable :: A, B\n",
    "    real   (kind=REAL64)                            :: summation, fortran_pi\n",
    "    integer(kind=INT32 )                            :: system_size\n",
    "    integer(kind=INT32 )                            :: i\n",
    "\n",
    "    fortran_pi  = acos(-1.0_real64)\n",
    "    summation   = 0.0_real64\n",
    "    system_size = 1e9\n",
    "    allocate(A(system_size), B(system_size))\n",
    "\n",
    "    !$acc data create(A(:), B(:))\n",
    "    !$acc parallel loop present(A(:), B(:))\n",
    "    do i = 1, system_size\n",
    "        A(i) = sin(i*fortran_pi/system_size) * sin(i*fortran_pi/system_size)\n",
    "        B(i) = cos(i*fortran_pi/system_size) * cos(i*fortran_pi/system_size)\n",
    "    enddo\n",
    "\n",
    "    call inplace_sum(A, B, system_size)\n",
    "\n",
    "    !$acc parallel loop present(A(:), B(:)) reduction(+:summation)\n",
    "    do i = 1, system_size\n",
    "        summation = summation + A(i)\n",
    "    enddo    \n",
    "    !$acc end data\n",
    "    write(0,\"(a29,f10.8)\") \"This should be close to 1.0: \", summation/dble(system_size)\n",
    "    deallocate(A, B)\n",
    "    contains\n",
    "        subroutine inplace_sum(A, B, n)\n",
    "        real   (kind=REAL64), dimension(:), intent(inout) :: A, B\n",
    "        integer(kind=INT32 ), intent(in)                  :: n\n",
    "        !$acc parallel loop present(A(:), B(:))\n",
    "        do i = 1, n\n",
    "            A(i) = A(i) + B(i)\n",
    "        enddo\n",
    "        end subroutine inplace_sum\n",
    "end program manual_build"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU Directives",
   "language": "python",
   "name": "gpu_directives"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
