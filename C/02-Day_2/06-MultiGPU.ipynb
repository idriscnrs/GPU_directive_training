{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f5f8e2",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Multi GPU programming with OpenACC\n",
    "\n",
    "---\n",
    "**Requirements:**\n",
    "\n",
    "- [Get started](./Get_started.ipynb)\n",
    "- [Data Management](./Data_management.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d64b",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "This part requires that you have a basic knowledge of OpenMP and/or MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf4aad",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "If you wish to have your code run on multiple GPUs, several strategies are available. The most simple ones are to create either several threads or MPI tasks, each one addressing one GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c25d08",
   "metadata": {
    "editable": false
   },
   "source": [
    "## API description\n",
    "\n",
    "For this part, the following API functions are needed:\n",
    "\n",
    "- *acc_get_device_type()*: retrieve the type of accelerator available on the host\n",
    "- *acc_get_num_device(device_type)*: retrieve the number of accelerators of the given type\n",
    "- *acc_set_device_num(id, device_type)*: set the id of the device of the given type to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d4fce",
   "metadata": {
    "editable": false
   },
   "source": [
    "## MPI strategy\n",
    "\n",
    "In this strategy, you will follow a classical MPI procedure where several tasks are executed. We will use either the OpenACC directive or API to make each task use 1 GPU.\n",
    "\n",
    "Have a look at the [examples/C/init_openacc.h](../../examples/C/init_openacc.h)\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/C/MultiGPU_mpi_example.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18517938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun -m 4 -a --gpus 2 --option \"-cpp\"\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "#include <openacc.h>\n",
    "#include \"../../examples/C/init_openacc.h\"\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    // Useful for OpenMPI and GPU DIRECT\n",
    "    initialisation_openacc();\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    // MPI Stuff\n",
    "    int my_rank;\n",
    "    int comm_size;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "    int a[100];\n",
    "    \n",
    "    // OpenACC Stuff\n",
    "    #ifdef _OPENACC\n",
    "    acc_device_t device_type = acc_get_device_type();\n",
    "    int num_gpus = acc_get_num_devices(device_type);\n",
    "    int my_gpu = my_rank%num_gpus;\n",
    "    acc_set_device_num(my_gpu, device_type);\n",
    "    my_gpu = acc_get_device_num(device_type);\n",
    "    // Alternatively you can set the GPU number with #pragma acc set device_num(my_gpu)\n",
    "    \n",
    "    #pragma acc parallel\n",
    "    {\n",
    "        #pragma acc loop\n",
    "        for(int i = 0; i< 100; ++i)\n",
    "            a[i] = i;\n",
    "    }\n",
    "    #endif\n",
    "    printf(\"Here is rank %d: I am using GPU %d of type %d. a[42] = %d\\n\", my_rank, my_gpu, device_type, a[42]);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a6299",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Remarks\n",
    "\n",
    "It is possible to have several tasks accessing the same GPU. It can be useful if one task is not enough to keep the GPU busy along the computation.\n",
    "\n",
    "If you use NVIDIA GPU, you should have a look at the [Multi Process Service](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2895b",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Multithreading strategy\n",
    "\n",
    "Another way to use several GPUs is with multiple threads. Each thread will use one GPU and several threads can share 1 GPU.\n",
    "\n",
    "Example stored in: `../../examples/C/MultiGPU_openmp_example.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c788c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun -a -t -g 4 --threads 4 --option \"-cpp\"\n",
    "#include <stdio.h>\n",
    "#include <openacc.h>\n",
    "#include <omp.h>\n",
    "int main(int argc,char** argv)\n",
    "{\n",
    "    #pragma omp parallel \n",
    "    {\n",
    "        int my_rank = omp_get_thread_num();\n",
    "        // OpenACC Stuff\n",
    "        #ifdef _OPENACC\n",
    "        acc_device_t dev_type = acc_get_device_type();\n",
    "        int num_gpus = acc_get_num_devices(dev_type);\n",
    "        int my_gpu = my_rank%num_gpus;\n",
    "        acc_set_device_num(my_gpu, dev_type);\n",
    "        // We check what GPU is really in use\n",
    "        my_gpu = acc_get_device_num(dev_type);\n",
    "        // Alternatively you can set the GPU number with #pragma acc set device_num(my_gpu)\n",
    "        printf(\"Here is thread %d: I am using GPU %d of type %d.\\n\", my_rank, my_gpu, dev_type);\n",
    "        #endif  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e9307",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Copy one cell from a previous notebook with a sequential code\n",
    "2. Modify the code to use several GPUs\n",
    "3. Check the correctness of the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c6813",
   "metadata": {
    "editable": false
   },
   "source": [
    "## GPU to GPU data transfers\n",
    "\n",
    "If you have several GPUs on your machine they are likely interconnected. For NVIDIA GPUs, there are 2 flavors of connections: either PCI express or NVlink.\n",
    "[NVLink](https://www.nvidia.com/fr-fr/data-center/nvlink/) is a fast interconnect between GPUs. Be careful since it might not be available on your machine.\n",
    "The main difference between the two connections is the bandwidth for CPU/GPU transfers, which is higher for NVlink.\n",
    "\n",
    "The GPUDirect feature of CUDA-aware MPI libraries allows direct data transfers between GPUs without an intermediate copy to the CPU memory. If you have access to an MPI CUDA-aware implementation with GPUDirect support, you should definitely adapt your code to benefit from this feature.\n",
    "\n",
    "For information, during this training course we are using OpenMPI which is CUDA-aware.\n",
    "You can find a list of CUDA-aware implementation on [NVIDIA website](https://developer.nvidia.com/mpi-solutions-gpus).\n",
    "\n",
    "By default, the data transfers between GPUs are not direct. The scheme is the following:\n",
    "\n",
    "1. The __origin__ task generates a Device to Host data transfer\n",
    "2. The __origin__ task sends the data to the __destination__ task.\n",
    "3. The __destination__ task generates a Host to Device data transfer\n",
    "\n",
    "Here we can see that 2 transfers between Host and Device are necessary. This is costly and should be avoided if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fe9e6",
   "metadata": {
    "editable": false
   },
   "source": [
    "### `acc host_data` directive\n",
    "\n",
    "To be able to transfer data directly between GPUs, we introduce the __host_data__ directive.\n",
    "```c\n",
    "#pragma acc host_data use_device(array)\n",
    "```\n",
    "\n",
    "\n",
    "This directive tells the compiler to assign the address of the variable to its value on the device.\n",
    "You can then use the pointer with your MPI calls.\n",
    "__You have to call the MPI functions on the host.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd727ea1",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here is a example of a code using GPU to GPU direct transfer.\n",
    "```c\n",
    "int size = 1000;\n",
    "int* array = (int*) malloc(size*sizeof(int));\n",
    "#pragma acc enter_data create(array[0:1000])\n",
    "// Perform some stuff on the GPU\n",
    "#pragma acc parallel present(array[0:1000])\n",
    "{\n",
    "...\n",
    "}\n",
    "// Transfer the data between GPUs\n",
    "if (my_rank == origin )\n",
    "{\n",
    "    #pragma acc host_data use_device(array)\n",
    "    MPI_Send(array, size, MPI_INT, destination, tag, MPI_COMM_WORLD);\n",
    "}\n",
    "else if (my_rank == destination)\n",
    "{\n",
    "    #pragma acc host_data use_device(array)\n",
    "    MPI_Recv(array, size, MPI_INT, origin, tag, MPI_COMM_WORLD, &status);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0115d27",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "As an exercise, you can complete the following MPI code that measures the bandwidth between the GPUs:\n",
    "\n",
    "1. Add directives to create the buffers on the GPU\n",
    "2. Measure the effective bandwidth between GPUs by adding the directives necessary to transfer data from one GPU to another one in the following cases:\n",
    "\n",
    "- Not using NVLink\n",
    "- Using NVLink\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/C/MultiGPU_mpi_exercise.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62377c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun -m 4 -a --gpus 2 --option \"-cpp\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <openacc.h>\n",
    "#include <math.h>\n",
    "#include \"../../examples/C/init_openacc.h\"\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    initialisation_openacc();\n",
    "    MPI_Init(&argc, &argv);\n",
    "    fflush(stdout);\n",
    "    double start;\n",
    "    double end;\n",
    "    \n",
    "    int size = 2e8/8;\n",
    "    \n",
    "    double* send_buffer = (double*)malloc(size*sizeof(double));\n",
    "    double* receive_buffer = (double*)malloc(size*sizeof(double));\n",
    "    // MPI Stuff\n",
    "    int my_rank;\n",
    "    int comm_size;\n",
    "    int reps = 5;\n",
    "    double data_volume = (double)reps*(double)size*sizeof(double)*pow(1024,-3.0);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "    MPI_Status status;\n",
    "    \n",
    "    // OpenACC Stuff\n",
    "    acc_device_t device_type = acc_get_device_type();\n",
    "    int num_gpus = acc_get_num_devices(device_type);\n",
    "    int my_gpu = my_rank%num_gpus;\n",
    "    acc_set_device_num(my_gpu, device_type); \n",
    "    for (int i = 0; i<comm_size; ++i)\n",
    "    {\n",
    "        for (int j=0; j < comm_size; ++j)\n",
    "        {\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                start = MPI_Wtime();\n",
    "                for (int k = 0 ; k < reps; ++k)\n",
    "                    MPI_Ssend(send_buffer, size, MPI_DOUBLE, j, 0, MPI_COMM_WORLD);\n",
    "            }\n",
    "            if (my_rank == j && i != j)\n",
    "            {\n",
    "                for (int k = 0 ; k < reps; ++k)\n",
    "                    MPI_Recv(receive_buffer, size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                end = MPI_Wtime();\n",
    "                printf(\"bandwidth %d->%d: %10.5f GB/s\\n\", i, j, data_volume/(end-start));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bc8ac",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### Solution\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/C/MultiGPU_mpi_solution.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7db876",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%idrrun -m 4 -a --gpus 2 --option \"-cpp\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <openacc.h>\n",
    "#include <math.h>\n",
    "#include \"../../examples/C/init_openacc.h\"\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    initialisation_openacc();\n",
    "    MPI_Init(&argc, &argv);\n",
    "    fflush(stdout);\n",
    "    double start;\n",
    "    double end;\n",
    "    \n",
    "    int size = 2e8/8;\n",
    "    \n",
    "    double* send_buffer = (double*)malloc(size*sizeof(double));\n",
    "    double* receive_buffer = (double*)malloc(size*sizeof(double));\n",
    "    #pragma acc enter data create(send_buffer[:size], receive_buffer[:size])\n",
    "    // MPI Stuff\n",
    "    int my_rank;\n",
    "    int comm_size;\n",
    "    int reps = 5;\n",
    "    double data_volume = (double)reps*(double)size*sizeof(double)*pow(1024,-3.0);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "    MPI_Status status;\n",
    "    \n",
    "    // OpenACC Stuff\n",
    "    acc_device_t device_type = acc_get_device_type();\n",
    "    int num_gpus = acc_get_num_devices(device_type);\n",
    "    int my_gpu = my_rank%num_gpus;\n",
    "    acc_set_device_num(my_gpu, device_type); \n",
    "    for (int i = 0; i<comm_size; ++i)\n",
    "    {\n",
    "        for (int j=0; j < comm_size; ++j)\n",
    "        {\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                start = MPI_Wtime();\n",
    "                #pragma acc host_data use_device(send_buffer)\n",
    "                for (int k = 0 ; k < reps; ++k)\n",
    "                    MPI_Ssend(send_buffer, size, MPI_DOUBLE, j, 0, MPI_COMM_WORLD);\n",
    "            }\n",
    "            if (my_rank == j && i != j)\n",
    "            {\n",
    "                #pragma acc host_data use_device(receive_buffer)\n",
    "                for (int k = 0 ; k < reps; ++k)\n",
    "                    MPI_Recv(receive_buffer, size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                end = MPI_Wtime();\n",
    "                printf(\"bandwidth %d->%d: %10.5f GB/s\\n\", i, j, data_volume/(end-start));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU Directives",
   "language": "python",
   "name": "gpu_directives"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
