{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b72326",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Introduction to GPU programming with directives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb888f7",
   "metadata": {
    "editable": false
   },
   "source": [
    "## What is a GPU?\n",
    "\n",
    "Graphical Processing Units (GPU) have been designed to accelerate the processing of graphics and have boomed thanks to video games which require more and more computing power.\n",
    "\n",
    "For this course we will use the terminology from NVIDIA.\n",
    "\n",
    "GPUs have a large number of computing core really efficient to process large matrices.\n",
    "For example, the latest generation of NVIDIA GPU (Hopper 100) have 132 processors, called Streaming Multiprocessors (SM) with different kinds of specialized cores:\n",
    "\n",
    "| Core Type | Number per SM |\n",
    "|-----------|---------------|\n",
    "|      FP32 |           128 |\n",
    "|      FP64 |            64 |\n",
    "|     INT32 |            64 |\n",
    "| TensorCore|             4 |\n",
    "|     Total |           260 |\n",
    "\n",
    "It means that you have roughly 34k cores on one GPU.\n",
    "\n",
    "At max, CPUs can have a few 10s of cores (AMD Epyc Rome have 64 cores).\n",
    "The comparison with the number of cores on one CPU is not fully relevant since their architecture differs a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cb339",
   "metadata": {
    "editable": false
   },
   "source": [
    "This is the scheme for one streaming multiprocessor in an [NVIDIA H100 GPU](https://resources.nvidia.com/en-us-tensor-core).\n",
    "\n",
    "<img alt=\"NVIDIA H100 GPU architecture\" src=\"../../pictures/H100.png\" style=\"float:none\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61d530",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Programming models\n",
    "\n",
    "<img alt=\"Programming models for GPUs\" src=\"../../pictures/models.png\" style=\"float:none\" width=\"50%\"/>\n",
    "\n",
    "You have the choice between several programming models to port your code to GPU:\n",
    "\n",
    "- Low level programming language (CUDA, OpenCL)\n",
    "- Programming models (Kokkos)\n",
    "- GPU libraries (CUDA accelerated libraries, MAGMA, THRUST, AmgX)\n",
    "- Directives languages (OpenACC, OpenMP target)\n",
    "\n",
    "Most of the time they are interoperable and you can get the best of each world as long as you take enough time to learn everything :).\n",
    "\n",
    "In this training course we focus on the directives languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41f717",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Low level programming languages: CUDA, OpenCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa64c9",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### CUDA\n",
    "\n",
    "Introduction of floating-point processing and programming capabilities on GPU cards at the turn of the century opened the door to general purpose GPU (GPGPU) programming. GPGPU was greatly democratized with the arrival of the [CUDA programming language](https://docs.nvidia.com/cuda/index.html) in 2007.\n",
    "\n",
    "CUDA is a language close to C++ where you have to manage yourself everything that occurs on the GPU:\n",
    "\n",
    "- Allocation of memory\n",
    "- Data transfers\n",
    "- Kernel (piece of code running on the GPU) execution\n",
    "\n",
    "The kernel configuration has to be explicitly written in your code.\n",
    "\n",
    "```c\n",
    "   __global__ void dot(float* a, float* b, float *c) {\n",
    "\n",
    "    __shared__ float temp[BLOCK_SIZE];\n",
    "    int idx = threadIdx.x + blockDim.x * blockIdx.x; // This calculates the global index of the current thread.\n",
    "    \n",
    "    temp[threadIdx.x] = a[idx] * b[idx]; \n",
    "    __syncthreads(); // This synchronizes all threads in the block.\n",
    "    \n",
    "    if (threadIdx.x == 0) { // Only the first thread computes the sum\n",
    "        float sum = 0.0; \n",
    "        for(int i = 0; i < BLOCK_SIZE; ++i)\n",
    "            sum += temp[i];\n",
    "        atomicAdd(c, sum); // This adds the sum to the value pointed by the pointer \"c\".\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "All of this means that if you want to port your code on GPU with CUDA you have to write specialized portions of code.\n",
    "With this you have access to potentially the full processing power of the GPU but you have to learn a new language.\n",
    "\n",
    "Since it is only available on NVIDIA GPUs you lack the portability to other platforms.\n",
    "\n",
    "Using [HIP](https://rocm.docs.amd.com/projects/HIP/) is a portable alternative to relying on CUDA. HIP is developped by AMD. It comes with a ROCm backend for AMD GPUs and a CUDA backend for Nvidia GPUs. There are currently efforts on adding to [HIP backends for Intel GPUs](https://github.com/CHIP-SPV/chipStar). The overhead of using HIP API on Nvidia is minimal. Syntactically, HIP is close to CUDA ; there exists a tool for CUDA to HIP conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1db1dc",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### OpenCL\n",
    "\n",
    "[OpenCL](https://www.khronos.org/opencl/) have been available since 2009 and it was developed to write code that can run on several kind of architectures (CPU, GPU, FPGA, ...).\n",
    "\n",
    "OpenCL is supported by the major hardware companies so if you choose this option you can alleviate the portability issue.\n",
    "However, you still have to manage by hand everything happening on the GPU.\n",
    "\n",
    "```c\n",
    "__kernel void dot(__global float* a, __global float* b, __global float *c) {\n",
    "    const int BLOCK_SIZE = 512;\n",
    "    __local float temp[BLOCK_SIZE];\n",
    "    int idx = get_global_id(0); // This calculates the global index of the current thread.\n",
    "    temp[get_local_id(0)] = a[idx] * b[idx];\n",
    "    barrier(CLK_LOCAL_MEM_FENCE); // This synchronizes all threads in the block.\n",
    "    if (get_local_id(0) == 0) { // Only the first thread computes the sum\n",
    "        float sum = 0.0;\n",
    "       for(int i = 0; i < BLOCK_SIZE; ++i)\n",
    "            sum += temp[i];\n",
    "        AtomicAdd(&c[0], sum); // This adds the sum to the value pointed by the pointer c.\n",
    "   }\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53e961",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Using libraries\n",
    "\n",
    "Let say that your code is spending a lot of time in only one type of computation (linear algebra, FFTs, etc).\n",
    "Then it is interesting to look for specialized libraries developed for this kind of computation:\n",
    "\n",
    "- [NVIDIA CUDA libraries](https://docs.nvidia.com/#nvidia-cuda-libraries): FFT, BLAS, Sparse algebra, ...\n",
    "- [MAGMA](https://icl.cs.utk.edu/magma/): Dense linear algebra\n",
    "- etc\n",
    "\n",
    "The implementation cost is much lower than if you have to write your own kernels and you get (hopefully) very good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5cecd",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Directives\n",
    "\n",
    "In the general case where the libraries do not fulfill an important part of your code, you can choose to use [OpenACC](https://www.openacc.org/) or [OpenMP 4.5 and above](https://www.openmp.org/) with the target construct.\n",
    "\n",
    "With this approach you annotate your code with directives considered as comments if you do not activate the compiler options to use them.\n",
    "\n",
    "For OpenACC:\n",
    "\n",
    "```c\n",
    "#pragma acc parallel loop\n",
    "for (int i=0; i<size; ++i)\n",
    "{\n",
    "    // Code to offload to GPU\n",
    "}\n",
    "```\n",
    "\n",
    "For OpenMP target:\n",
    "\n",
    "```c\n",
    "#pragma omp target teams distribute parallel for\n",
    "for (int i=0; i<size; ++i)\n",
    "{\n",
    "    // Code to offload to GPU\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57daf8",
   "metadata": {
    "editable": false
   },
   "source": [
    "The implementation cost is much lower than the low level programming languages and usually you can get up to 95% of the performance you would get by writing your own specialized code.\n",
    "\n",
    "Even though the modifications in your code will be lower than rewriting everything, you have to keep in mind that some changes might be necessary to have the best performance possible.\n",
    "Those changes can be in:\n",
    "\n",
    "- the algorithms\n",
    "- the data structures\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cdf2f",
   "metadata": {
    "editable": false
   },
   "source": [
    "## OpenACC\n",
    "\n",
    "The first version of the OpenACC specification was released in November 2011.\n",
    "It was created by:\n",
    "\n",
    "- Cray\n",
    "- NVIDIA\n",
    "- PGI (now part of NVIDIA)\n",
    "- CAPS\n",
    "\n",
    "In November 2023 they released the [3.3 specification](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.3-final.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3490bb",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Compilers for OpenACC\n",
    "\n",
    "Several [compilers](https://www.openacc.org/tools) are available to produce OpenACC code:\n",
    "\n",
    "- [Cray Programming environment](https://pubs.cray.com/category/pe-tile) (for Cray hardware)\n",
    "- [NVIDIA HPC SDK](https://developer.nvidia.com/hpc-sdk) (formerly PGI)\n",
    "- [GCC 12](https://gcc.gnu.org/gcc-12/) : available since version 10\n",
    "- [AMD Sourcery CodeBench](https://docs.amd.com/r/en-US/ug821-zynq-7000-swdev/Sourcery-CodeBench-Lite-Edition-for-AMD-Cortex-A9-Compiler-Toolchain)\n",
    "- etc\n",
    "\n",
    "You have to be careful since the maturity of each compiler and the specification they respect can change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d55b8b",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### Disclaimer\n",
    "\n",
    "The training course is based on version 2.7 of the specification.\n",
    "\n",
    "Here we will mainly use the HPC compilers from NVIDIA available on their [website](https://developer.nvidia.com/hpc-sdk) which fully respects [specification 2.7](https://www.openacc.org/sites/default/files/inline-files/OpenACC.2.7.pdf).\n",
    "You will be able to test the GCC compilers which supports [specification 2.6](https://www.openacc.org/sites/default/files/inline-files/OpenACC.2.6.final.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52381e",
   "metadata": {
    "editable": false
   },
   "source": [
    "## OpenMP target\n",
    "\n",
    "The first OpenMP specification which supports GPU offloading is [4.5](https://www.openmp.org/wp-content/uploads/openmp-4.5.pdf) released in November 2015.\n",
    "It adds the `target` construct for this purpose.\n",
    "\n",
    "The newest specification (november 2021) for OpenMP is [5.2](https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65fd7f",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Compilers for OpenMP target\n",
    "\n",
    "The list of compilers supporting OpenMP is available on the [OpenMP website](https://www.openmp.org/resources/openmp-compilers-tools/).\n",
    "You have to check if the `target` (or offloading) is supported.\n",
    "\n",
    "The main compilers which support offloading to GPU are:\n",
    "\n",
    "- IBM XL for [C/C++](https://www.ibm.com/products/c-and-c-plus-plus-compiler-family) and [Fortran](https://www.ibm.com/products/xl-fortran-linux-compiler-power)\n",
    "- [GCC since version 7](https://gcc.gnu.org/)\n",
    "- [CLANG](https://clang.llvm.org/)\n",
    "- [NVIDIA HPC SDK](https://developer.nvidia.com/hpc-sdk) (formerly PGI)\n",
    "- [Cray Programming environment](https://pubs.cray.com/category/pe-tile) (for Cray hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a21a2",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Host driven Language\n",
    "\n",
    "OpenACC is a host driven programming language.\n",
    "It means that the host (usually a CPU) is in charge of launching everything happening on the device (usually a GPU) including:\n",
    "\n",
    "- Executing kernels\n",
    "- Memory allocations\n",
    "- Data transfers\n",
    "\n",
    "<img alt=\"The CPU controls the GPU\" src=\"../../pictures/CPUcontrolGPU.png\" style=\"float:none\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1d433",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Levels of parallelism\n",
    "\n",
    "On the GPU you can have 4 different levels of parallelism that can be activated:\n",
    "\n",
    "- Coarse grain: gang\n",
    "- Fine grain : worker\n",
    "- Vectorization : vector\n",
    "- Sequential : seq\n",
    "\n",
    "One Gang is made of several Workers which are vectors (with by default a size of one thread).\n",
    "You can increase the number of thread by activating the Vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8540c2b",
   "metadata": {
    "editable": false
   },
   "source": [
    "Inside a kernel gangs have the same number of threads running.\n",
    "But it can be different from one kernel to another.\n",
    "\n",
    "So the total number of threads used by a kernel is $(Number\\_of\\_Gangs) * (Number\\_of\\_Workers) * (Vector\\_Length)$.\n",
    "\n",
    "<img alt=\" Levels of parallelism in OpenACC\" src=\"../../pictures/examples_SM_gang_worker_vector.png\" style=\"float:none\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e4f7b",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Important notes\n",
    "\n",
    "- There is no way to synchronize threads between gangs.\n",
    "- The compiler may decide to add synchronization within the threads in one gang.\n",
    "- The threads of a worker work in [SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) mode.\n",
    "  It means that all threads run the same instruction at the same time.\n",
    "  For example on NVIDIA GPUS, groups of 32 threads are formed.\n",
    "- Usually NVIDIA compilers set the number of workers to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2a715",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Information about NVIDIA devices\n",
    "\n",
    "The `nvaccelinfo` command gives interesting information about the devices available.\n",
    "\n",
    "For example, if you run it on Jean Zay A100 partition.\n",
    "\n",
    "```bash\n",
    "$ nvaccelinfo\n",
    "\n",
    "Device Number:                 7\n",
    "Device Name:                   NVIDIA A100-SXM4-80GB\n",
    "Device Revision Number:        8.0\n",
    "Global Memory Size:            85051572224\n",
    "Number of Multiprocessors:     108\n",
    "Concurrent Copy and Execution: Yes\n",
    "Total Constant Memory:         65536\n",
    "Total Shared Memory per Block: 49152\n",
    "Registers per Block:           65536\n",
    "Warp Size:                     32\n",
    "Maximum Threads per Block:     1024\n",
    "Maximum Block Dimensions:      1024, 1024, 64\n",
    "Maximum Grid Dimensions:       2147483647 x 65535 x 65535\n",
    "Maximum Memory Pitch:          2147483647B\n",
    "Clock Rate:                    1410 MHz\n",
    "Concurrent Kernels:            Yes\n",
    "Memory Clock Rate:             1593 MHz\n",
    "L2 Cache Size:                 41943040 bytes\n",
    "Max Threads Per SMP:           2048\n",
    "Async Engines:                 3\n",
    "Managed Memory:                Yes\n",
    "Default Target:                cc80\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU Directives",
   "language": "python",
   "name": "gpu_directives"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
