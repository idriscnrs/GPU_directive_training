{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbe7730",
   "metadata": {
    "editable": false
   },
   "source": [
    "# OpenMP offloading directives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610eb81",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Directives\n",
    "\n",
    "OpenMP since specification 4.5 includes support for offloading to accelerators like GPUs.\n",
    "It uses directives to do so (just like for CPU).\n",
    "\n",
    "A directive has the following structure:\n",
    "\n",
    "<img src=\"../../pictures/directive_omp.png\" style=\"float:none\" width=\"30%\"/>\n",
    "\n",
    "If we break it down, we have these elements:\n",
    "\n",
    "- The sentinel is special instruction for the compiler. It tells it that what follows has to be interpreted as OpenACC\n",
    "- The directive is the action to do. In the example, _target_ is the way to open a region that will be offloaded to the GPU\n",
    "- The clauses are \"options\" of the directive. In the example we want to copy some data on the GPU.\n",
    "- The clause arguments give more details for the clause. In the example, we give the name of the variables to be copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d134f4",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Compiling with NVIDIA compiler\n",
    "\n",
    "To enable OpenMP GPU offloading you need to activate the compilation options `-mp=gpu -gpu=<gpu,opts>`.\n",
    "For example to compile for NVIDIA V100:\n",
    "\n",
    "```bash\n",
    "nvc -mp=gpu -gpu=cc70 -o test test.f90\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79a61e",
   "metadata": {
    "editable": false
   },
   "source": [
    "## GPU offloading\n",
    "\n",
    "With OpenMP the offloading is realized with the `omp target` directive.\n",
    "By itself, the directive will only offload the computation and do not activate parallelism.\n",
    "It is similar to the `acc serial` compute construct in OpenACC since only one GPU thread is running.\n",
    "\n",
    "With OpenMP the developer has to activate manually the parallelism.\n",
    "\n",
    "Here is an example on how to create a GPU kernel:\n",
    "```c\n",
    "#pragma omp target\n",
    "{\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "Now that we run on the GPU we have to create the threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ae4fb",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Thread creation on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a3e78",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Teams\n",
    "\n",
    "OpenMP `target teams` directive creates several groups of threads that will be able to work in parallel.\n",
    "\n",
    "<span style=\"font-size: .8rem\"> With OpenACC it would correspond to the `gang` level of parallelism.</span>\n",
    "\n",
    "```c\n",
    "#pragma omp target teams\n",
    "{\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "By default the teams will work in replicated mode meaning that they will perform exactly the same things.\n",
    "If you want to share the iterations of a loop between the threads of the teams you have to use the `teams distribute` directive.\n",
    "\n",
    "```c\n",
    "#pragma omp target\n",
    "{\n",
    "    #pragma omp teams distribute\n",
    "    for (int i=0; i<size; ++u)\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This will split the iterations of the loop among the teams. Each team will have a contiguous set of iterations.\n",
    "\n",
    "It starting to be interesting but we do not yet take advantage of the full power of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafea283",
   "metadata": {
    "editable": false
   },
   "source": [
    "### More threads with `omp parallel`\n",
    "\n",
    "With the `omp parallel` directive inside a `omp teams` region we create the threads that will be used inside the team.\n",
    "\n",
    "```c\n",
    "#pragma omp target teams distribute parallel\n",
    "for (int i=0; i<sys_size; ++i)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this case the threads generated inside the teams will work in replicated mode.\n",
    "If we want to further split the work among those threads we have to add the `omp do` (Fortran) or `omp for` (C/C++) directive.\n",
    "\n",
    "```c\n",
    "#pragma omp target teams distribute parallel do\n",
    "for (int i=0; i<sys_size; ++i)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "<span style=\"font-size: .8rem\"> With OpenACC it would correspond to the `worker` level of parallelism.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bf4d5",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Let's vectorize with `omp simd`\n",
    "\n",
    "The last level of parallelism we can leverage with OpenMP is the SIMD vectorization.\n",
    "It is done with the `omp simd` directive:\n",
    "\n",
    "```c\n",
    "#pragma omp target teams distribute parallel do simd\n",
    "for (int i=0; i<sys_size; ++i)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc492e2",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### Note for NVIDIA compilers\n",
    "\n",
    "The `omp simd` construct is not supported for GPU. Currently, the `parallel` directive creates the threads that should be created with `simd`.\n",
    "Since the directive is just ignored, we recommend that you write it for portability reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07431c",
   "metadata": {
    "editable": false
   },
   "source": [
    "### `collapse` clause\n",
    "\n",
    "The  `collapse` clause enables to merge all the iterations of several associated loops into a single large iteration loop. The number of loops that will be merged is indicated as an integer argument to this clause and should be greater than 1.\n",
    "\n",
    "```c\n",
    "#pragma omp target teams distribute parallel for simd collapse(3)\n",
    "for (int i=0;i<nx;i++)\n",
    "    for (int i=0;i<nx;i++)\n",
    "        for (int i=0;i<nx;i++)\n",
    "           ...\n",
    "```\n",
    "\n",
    "Up to now, we will recommend you to use the collapse clause as much as you can with OpenMP target in order to achieve good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b8809",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Example\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_basic_offloading.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1bdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    int size = 100000000;\n",
    "    double* array = (double*) malloc(size*sizeof(double));\n",
    "\n",
    "// We need to explicitly manage transfers in C with NVIDIA compilers\n",
    "// otherwise we get a runtime error (as of nvhpc 21.9)\n",
    "#pragma omp target teams distribute parallel for simd map(from:array[0:size])\n",
    "    for (int i=0; i<size; ++i)\n",
    "        array[i] = (double) i;\n",
    "\n",
    "    printf(\"array[42] = %f\\n\", array[42]);\n",
    "    free(array);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62034e0",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Reductions\n",
    "\n",
    "Reductions should be performed when a memory location is updated by several threads concurrently, and usually prior to its previous value.\n",
    "\n",
    "This can be performed by using the `reduction` clause of the target construct. This clause will create a private copy of the variables and initialize them as a function of the requested reduction operation. Once you reach the end of the kernel, the original variable will be updated with a combination of all the private copies.\n",
    "\n",
    "The syntax is:\n",
    "\n",
    "```c\n",
    "#pragma omp target parallel for reduction(operation:variable_list)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "The available operations are:\n",
    "\n",
    "- +, -\n",
    "- \\*\n",
    "- &, |, ^, &&, ||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07b0f2",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Limitation\n",
    "\n",
    "The reductions are now only supported for the 2 following combined constructs:\n",
    "\n",
    "- `omp target parallel for`\n",
    "- `omp target teams distribute parallel for`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e67463",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada33401",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Implicit behavior\n",
    "\n",
    "If not specified in a `data map` structure, variables will be mapped implicitly at the entry of one kernel with a default action depending on the type of the variable.\n",
    "\n",
    "Scalars will be map as `firstprivate`, i.e. every thread will have its own private copy that will be initialized with the value that the scalar have on the CPU before the kernel.\n",
    "\n",
    "Arrays will be shared in memory between threads and are implicitly mapped as if you specified `map(tofrom:)`.\n",
    "\n",
    "Pointers will be private by default.\n",
    "\n",
    "You can see the effect of this implicit behavior with the example below:\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_Implicit_behavior.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    int size = 10;\n",
    "    double* array = (double*) malloc(size*sizeof(double));\n",
    "    double  scalar;\n",
    "\n",
    "    scalar = 1000.0;\n",
    "#pragma omp target teams distribute parallel for\n",
    "    for (int i=0; i<size; ++i)\n",
    "        array[i] = (double)i + scalar;\n",
    "\n",
    "    for (int i=0; i<size; ++i)\n",
    "        printf(\"%lf\\n\",array[i]);\n",
    "\n",
    "    scalar = -1000.1;\n",
    "\n",
    "#pragma omp target teams distribute parallel for\n",
    "    for (int i=0; i<size; ++i)\n",
    "        array[i] = (double)i + scalar;\n",
    "\n",
    "    for (int i=0; i<size; ++i)\n",
    "        printf(\"%lf\\n\",array[i]);\t    \n",
    "\n",
    "    free(array);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d43ab",
   "metadata": {},
   "source": [
    "Relying only on the implicit behavior can lead to performance degradation as data transfers are performed back and forth at every kernels. This should be avoid by using data regions.\n",
    "\n",
    "You can define a specific action to perform at the entry and/or the exit of a kernel for a variable or a set of variable with the `map` clause of the `target` construct.\n",
    "\n",
    "The available options are:\n",
    "\n",
    "- `alloc` to create the memory space of the variables without prior data transfer.\n",
    "- `to` to create the memory space of the variables and transfer the values from CPU to GPU at the entry of the kernel.\n",
    "- `from` to create the memory space of the variables and transfer the values from GPU to CPU at the exit of the kernel.\n",
    "- `tofrom` to create the memory space of the variables and transfer the values from CPU to GPU at the entry of the kernel, then from GPU to CPU at the exit.\n",
    "\n",
    "<img src=\"../../pictures/data_clauses_omp.png\" style=\"float:none\" width=\"65%/\"/>\n",
    "\n",
    "The syntax is:\n",
    "\n",
    "```c\n",
    "#pragma omp target map(from:variable1,variable2)\n",
    "{\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "It is also possible to modify the status of the variable manually with the `private` and `firstprivate` clauses of the `target` construct or by setting a default mapping that we will see later.\n",
    "\n",
    "```c\n",
    "#pragma omp target private(variable1,variable2) firstprivate(variable3)\n",
    "{\n",
    "    ...\n",
    "    // variable1 and variable2 will have independent memory allocations for each threads\n",
    "    // variable2 will have independent memory allocations for each threads and will be initialized with the CPU value\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b120db",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Structured data region\n",
    "\n",
    "To run the kernels on GPU, the data should be allocated on the device and eventually the original values should be transfered from the CPU to the GPU.\n",
    "You will also have to retrieve some of the data back from the GPU to the CPU in order to store your results.\n",
    "This can be perfomed withing the same program unit by using the `target data` construct.\n",
    "\n",
    "If you don't use data regions, implicit copies of the variables will be performed at each entry and exit of every kernels.\n",
    "This implies transfers trough the PCIe that could be avoided and thus non-optimal performances.\n",
    "\n",
    "This construct map the variable to the device, but only for the extent of the region.\n",
    "The `map` clause enables you to decide which action will be performed on the gpu.\n",
    "These actions could be `alloc`, `to`, `from`, `tofrom`.\n",
    "\n",
    "You can retrieve the values that were stored on the GPU with `from` and `tofrom` clauses\n",
    "\n",
    "You can inform the GPU of the original CPU values with the clauses `to` and `tofrom`.\n",
    "\n",
    "If you use the `alloc` or `from` clause, the initial value on the device is undetermined.\n",
    "\n",
    "The syntax is:\n",
    "\n",
    "```c\n",
    "    double* A = (double*) malloc(nx*ny*sizeof(double));\n",
    "    double* B = (double*) malloc(nx*ny*sizeof(double));\n",
    "    #pragma omp target data map(tofrom:A[0:nx*ny], B[0:nx*ny])\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a64d4",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Persistent data (`enter data` / `exit data`)\n",
    "\n",
    "If you want to allocate the memory of some variables on the device at a given point of your program but it is not possible to free the memory within the same scope of the program,\n",
    "you can then use the `enter data` and `exit data` constructs.\n",
    "\n",
    "`enter data` will enable you to allocate or allocate and initialize the variables on the GPU with the `map(alloc:variable_list)` and `map(to:variable_list)` clauses respectively.\n",
    "\n",
    "`exit data` will enable you to free the memory from the device, resp. free the memory after retrieving the data, with the `map(delete:variable_list)`, resp. `map(from:variable_list)`.\n",
    "\n",
    "These 2 constructs are not tied to each other, such as one `enter data` construct mapping several variables can lead to several `exit data` constructs in different portions of the code as long as 2 `exit data` are not refering to the same variable in this example.\n",
    "\n",
    "The syntax is:\n",
    "\n",
    "```c\n",
    "void some_function_somewhere(void)\n",
    "{\n",
    "    double* A = (double*) malloc(nx*ny*sizeof(double));\n",
    "    double* B = (double*) malloc(nx*ny*sizeof(double));\n",
    "    #pragma omp target enter data map(to:A[0:nx*ny])\n",
    "    #pragma omp target enter data map(alloc:B[0:nx*ny])\n",
    "        ...\n",
    "}\n",
    "\n",
    "void some_function_elsewhere_or_maybe_the_same_as_before(void)\n",
    "{\n",
    "    ...\n",
    "    #pragma omp target exit data map(delete:A, B)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97949bd6",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Manual data tranfers\n",
    "\n",
    "When you want to update the values of a given variable, or a set of variables, either on the GPU or on the CPU, you can use the `target update` construct in order to avoid doing it by closing a data structure.\n",
    "\n",
    "The `to` clause will update the GPU.\n",
    "\n",
    "The `from` clause will update the CPU.\n",
    "\n",
    "```c\n",
    "#pragma omp target update to(picture[0:num_elements])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7096f",
   "metadata": {
    "editable": false
   },
   "source": [
    "### `defaultmap` clause\n",
    "\n",
    "You can modify the default mapping for the data transfer upon kernels or data structures with the `defaultmap` clause of the `target` and `target data` constructs.\n",
    "\n",
    "The new implicit behavior can be specified as `alloc`, `to`, `from`, `tofrom`, `default`, `none`, `firstprivate` or `present` and should be applied to a variable category. Variable categories are:\n",
    "\n",
    "- scalar\n",
    "- aggregate (corresponding to arrays and structures in C/C++ and to derived types in Fortran)\n",
    "- allocatable (only for Fortran arrays that are dynamically allocated)\n",
    "- pointers\n",
    "\n",
    "If you specify the implicit behavior as `none`, you should then map explicitly all variables.\n",
    "\n",
    "```c\n",
    "int A[N];\n",
    "int B;\n",
    "\n",
    "#pragma omp target defaultmape(firstprivate:scalar) defaultmap(tofrom:aggregate)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a5b9c",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Modular programming\n",
    "\n",
    "Functions that are call inside a kernel should be executed on the accelerator. You should use the `declare target` construt to inform the compiler that it should produce such an executable. Syntax should be:\n",
    "\n",
    "```c\n",
    "#pragma omp declare target\n",
    "void my_funtion(void)\n",
    "{\n",
    "        ...\n",
    "}\n",
    "#pragma omp end declare target\n",
    "```\n",
    "\n",
    "\n",
    "If the function and the line from which the function is called are not within the same program unit, you should add a named `declare target` construct within the program unit containing the call.\n",
    "\n",
    "```c\n",
    "#pragma omp declare target\n",
    "void my_function(void){\n",
    "    ...\n",
    "}\n",
    "#pragma omp end declare target\n",
    "\n",
    "\n",
    "int main(void){\n",
    "    #pragma declare target(my_function)  \n",
    "    ...\n",
    "    #pragma target teams distribute\n",
    "    {\n",
    "    a = my_function();\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46eeff5",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_Modular_programming_mean_value_exercise.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6acb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "double mean_value(double* array, size_t array_size){\n",
    "    double sum = 0.0;\n",
    "    for(size_t i=0; i<array_size; ++i)\n",
    "        sum += array[i];\n",
    "    return sum/array_size;\n",
    "}\n",
    "\n",
    "void rand_init(double* array, size_t array_size)\n",
    "{\n",
    "     srand((unsigned) 12345900);\n",
    "     for (size_t i=0; i<array_size; ++i)\n",
    "         array[i] = 2.*((double)rand()/RAND_MAX -0.5);\n",
    "}\n",
    "\n",
    "void iterate(double* array, size_t array_size, size_t cell_size)\n",
    "{\n",
    "    double local_mean;\n",
    "    for (size_t i = cell_size/2; i< array_size-cell_size/2; ++i)\n",
    "    {\n",
    "        local_mean = mean_value(&array[i-cell_size/2], cell_size);\n",
    "        if (local_mean < 0.)\n",
    "            array[i] += 0.1;\n",
    "        else if (local_mean > 0.)\n",
    "            array[i] -= 0.1;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    size_t num_cols = 10000;\n",
    "    size_t num_rows = 3000;\n",
    "\n",
    "    double* table = (double*) malloc(num_rows*num_cols*sizeof(double)); \n",
    "    double* mean_values = (double*) malloc(num_rows*sizeof(double));\n",
    "    // We initialize the first row with random values between -1 and 1\n",
    "    rand_init(table, num_cols);\n",
    "\n",
    "    for (size_t i=1; i<num_rows; ++i)\n",
    "       iterate(&table[i*num_cols], num_cols, 32); \n",
    "    \n",
    "    for (size_t i=0; i<num_rows; ++i) \n",
    "    {\n",
    "        mean_values[i] = mean_value(&(table[i*num_cols]), num_cols);\n",
    "    }\n",
    "\n",
    "    for (size_t i=0; i<10; ++i)\n",
    "        printf(\"Mean value of row %6d=%10.5f\\n\", i, table[i]);\n",
    "    printf(\"...\\n\");\n",
    "    for (size_t i=num_rows-10; i<num_rows; ++i)\n",
    "        printf(\"Mean value of row %6d=%10.5f\\n\", i, table[i]);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d4648",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Solution\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_Modular_programming_mean_value_solution.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8a31a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%idrrun --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#pragma omp declare target\n",
    "double mean_value(double* array, size_t array_size){\n",
    "    double sum = 0.0;\n",
    "    for(size_t i=0; i<array_size; ++i)\n",
    "        sum += array[i];\n",
    "    return sum/array_size;\n",
    "}\n",
    "#pragma omp end declare target\n",
    "\n",
    "void rand_init(double* array, size_t array_size)\n",
    "{\n",
    "     srand((unsigned) 12345900);\n",
    "     for (size_t i=0; i<array_size; ++i)\n",
    "         array[i] = 2.*((double)rand()/RAND_MAX -0.5);\n",
    "}\n",
    "\n",
    "void iterate(double* array, size_t array_size, size_t cell_size)\n",
    "{\n",
    "    double local_mean;\n",
    "    #pragma omp target teams distribute parallel for simd\n",
    "    for (size_t i = cell_size/2; i< array_size-cell_size/2; ++i)\n",
    "    {\n",
    "        local_mean = mean_value(&array[i-cell_size/2], cell_size);\n",
    "        if (local_mean < 0.)\n",
    "            array[i] += 0.1;\n",
    "        else if (local_mean > 0.)\n",
    "            array[i] -= 0.1;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    size_t num_cols = 10000;\n",
    "    size_t num_rows = 3000;\n",
    "\n",
    "    double* table = (double*) malloc(num_rows*num_cols*sizeof(double)); \n",
    "    double* mean_values = (double*) malloc(num_rows*sizeof(double));\n",
    "    // We initialize the first row with random values between -1 and 1\n",
    "    rand_init(table, num_cols);\n",
    "    #pragma omp target enter data map(to:table[0:num_rows*num_cols])\n",
    "\n",
    "    for (size_t i=1; i<num_rows; ++i)\n",
    "       iterate(&table[i*num_cols], num_cols, 32); \n",
    "    \n",
    "    #pragma omp target teams distribute parallel for simd map(from:mean_values[0:num_rows])\n",
    "    for (size_t i=0; i<num_rows; ++i) \n",
    "    {\n",
    "        mean_values[i] = mean_value(&(table[i*num_cols]), num_cols);\n",
    "    }\n",
    "\n",
    "    #pragma omp target exit data map(delete:table)\n",
    "    for (size_t i=0; i<10; ++i)\n",
    "        printf(\"Mean value of row %6d=%10.5f\\n\", i, table[i]);\n",
    "    printf(\"...\\n\");\n",
    "    for (size_t i=num_rows-10; i<num_rows; ++i)\n",
    "        printf(\"Mean value of row %6d=%10.5f\\n\", i, table[i]);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ab5df",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Using multiple GPUs with OpenMP\n",
    "\n",
    "If you have multiple accelerators available, you can select the one on which you run the kernels with the `device` clause of the `target` construct. It includes both `target data` constructs and `target teams/parallel` constructs.\n",
    "\n",
    "You should give an integer that refers to the gpu number (starting from 0) to the `device` clause, such as :\n",
    "\n",
    "```c\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    int num_gpus = omp_get_num_devices();\n",
    "    int my_gpu = my_rank%num_gpus\n",
    "    #pragma omp target data map(...) device(my_gpu)\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bada517",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "In this exercise, you should bring on the gpu the MPI version of the generation of the Mandelbrot set on the gpu with OpenMP and by using multiple devices.\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_mandelbrot_mpi_exercise.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c381422",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun  --cliopts \"2000 1000\" -m 4 -g 4 --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <complex.h>\n",
    "#include <mpi.h>\n",
    "#include <omp.h>\n",
    "void output(unsigned char* picture, unsigned int start, unsigned int num_elements)\n",
    "{\n",
    "   MPI_File     fh;\n",
    "   MPI_Offset   woffset=start;\n",
    "\n",
    "   if (MPI_File_open(MPI_COMM_WORLD,\"mandel.gray\",MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,&fh) != MPI_SUCCESS)\n",
    "   {\n",
    "        fprintf(stderr,\"ERROR in creating output file\\n\");\n",
    "        MPI_Abort(MPI_COMM_WORLD,1);\n",
    "   }\n",
    "\n",
    "   MPI_File_write_at(fh,woffset,picture,num_elements,MPI_UNSIGNED_CHAR,MPI_STATUS_IGNORE);\n",
    "\n",
    "   MPI_File_close(&fh);\n",
    "} \n",
    "\n",
    "unsigned char mandelbrot_iterations(const float complex c)\n",
    "{\n",
    "    unsigned char max_iter = 255;\n",
    "    unsigned char n = 0;\n",
    "    float complex z = 0.0 + 0.0 * I;\n",
    "    while (abs(z*z) <= 2 && n < max_iter)\n",
    "    {\n",
    "        z = z*z + c;\n",
    "        ++n;\n",
    "    }\n",
    "    return n;\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    unsigned int width = (unsigned int) atoi(argv[1]);\n",
    "    float step_w = 1./width;\n",
    "    unsigned int height = (unsigned int) atoi(argv[2]);\n",
    "    float step_h = 1./height;\n",
    "\n",
    "    const float min_re = -2.;\n",
    "    const float max_re = 1.;\n",
    "    const float min_im = -1.;\n",
    "    const float max_im = 1.;\n",
    "\n",
    "    struct timespec end, start;\n",
    "    clock_gettime(CLOCK_MONOTONIC_RAW, &start);\n",
    "\n",
    "    int i;\n",
    "    int rank;\n",
    "    int nb_procs;\n",
    "    int total_devices;\n",
    "    \n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &nb_procs);\n",
    "\n",
    "    unsigned int local_height = height / nb_procs;\n",
    "    unsigned int first = 0;\n",
    "    unsigned int last = local_height;\n",
    "    unsigned int rest_eucli = height % nb_procs;\n",
    "\n",
    "    if ((rank==0) && (rank < rest_eucli))\n",
    "          ++last;\n",
    "\n",
    "    for (i=1; i <= rank; ++i)\n",
    "    {\n",
    "      first += local_height;\n",
    "      last  += local_height;\n",
    "      if (rank < rest_eucli)\n",
    "          {\n",
    "              ++first;\n",
    "              ++last;\n",
    "          }\n",
    "    }\n",
    "\n",
    "    if (rank < rest_eucli) \n",
    "        ++local_height;\n",
    "\n",
    "    unsigned int num_elements = width*local_height;\n",
    "    if (rank == 0) printf(\"Using MPI\\n\");\n",
    "    total_devices = 0\n",
    "    printf(\"I am rank %2d and my range is [%5d, %5d[ ie %10d elements. Runing on %d GPUs.\\n\", rank, first, last, num_elements, total_devices);\n",
    "    unsigned char* restrict picture = (unsigned char*) malloc(num_elements*sizeof(unsigned char));\n",
    "    for (unsigned int i=0; i<local_height; ++i)\n",
    "        for (unsigned int j=0; j<width; ++j)\n",
    "        {\n",
    "            float complex c;\n",
    "            c = min_re + j*step_w * (max_re - min_re) + \\\n",
    "                I * (min_im +  ((i+first) * step_h) * (max_im - min_im));\n",
    "            picture[width*i+j] = (unsigned char) (255-rank*(255/nb_procs)) - mandelbrot_iterations(c);\n",
    "        }\n",
    "    output(picture, first*width, num_elements); \n",
    "    MPI_Finalize();\n",
    "\n",
    "    // Measure time\n",
    "    clock_gettime(CLOCK_MONOTONIC_RAW, &end);\n",
    "    unsigned long int delta_us = (end.tv_sec - start.tv_sec) * 1000000 + (end.tv_nsec - start.tv_nsec) / 1000;\n",
    "    printf(\"The time to generate the mandelbrot picture was %lu us\\n\", delta_us);\n",
    "    return EXIT_SUCCESS;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1589f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idrcomp import show_gray\n",
    "show_gray(\"mandel.gray\", 1000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fc3b8",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Solution\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_mandelbrot_mpi_solution.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bdc668",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%idrrun  --cliopts \"2000 1000\" -m 4 -g 4 --options \"-mp=gpu -gpu=cc70 -Minfo=all\" \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <complex.h>\n",
    "#include <mpi.h>\n",
    "#include <omp.h>\n",
    "void output(unsigned char* picture, unsigned int start, unsigned int num_elements)\n",
    "{\n",
    "   MPI_File     fh;\n",
    "   MPI_Offset   woffset=start;\n",
    "\n",
    "   if (MPI_File_open(MPI_COMM_WORLD,\"mandel.gray\",MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,&fh) != MPI_SUCCESS)\n",
    "   {\n",
    "        fprintf(stderr,\"ERROR in creating output file\\n\");\n",
    "        MPI_Abort(MPI_COMM_WORLD,1);\n",
    "   }\n",
    "\n",
    "   MPI_File_write_at(fh,woffset,picture,num_elements,MPI_UNSIGNED_CHAR,MPI_STATUS_IGNORE);\n",
    "\n",
    "   MPI_File_close(&fh);\n",
    "} \n",
    "\n",
    "#pragma omp declare target\n",
    "unsigned char mandelbrot_iterations(const float complex c)\n",
    "{\n",
    "    unsigned char max_iter = 255;\n",
    "    unsigned char n = 0;\n",
    "    float complex z = 0.0 + 0.0 * I;\n",
    "    while (abs(z*z) <= 2 && n < max_iter)\n",
    "    {\n",
    "        z = z*z + c;\n",
    "        ++n;\n",
    "    }\n",
    "    return n;\n",
    "}\n",
    "#pragma omp end declare target\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    unsigned int width = (unsigned int) atoi(argv[1]);\n",
    "    float step_w = 1./width;\n",
    "    unsigned int height = (unsigned int) atoi(argv[2]);\n",
    "    float step_h = 1./height;\n",
    "\n",
    "    const float min_re = -2.;\n",
    "    const float max_re = 1.;\n",
    "    const float min_im = -1.;\n",
    "    const float max_im = 1.;\n",
    "\n",
    "    struct timespec end, start;\n",
    "    clock_gettime(CLOCK_MONOTONIC_RAW, &start);\n",
    "\n",
    "    int i;\n",
    "    int rank;\n",
    "    int nb_procs;\n",
    "    int total_devices;\n",
    "    \n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &nb_procs);\n",
    "\n",
    "    unsigned int local_height = height / nb_procs;\n",
    "    unsigned int first = 0;\n",
    "    unsigned int last = local_height;\n",
    "    unsigned int rest_eucli = height % nb_procs;\n",
    "\n",
    "    if ((rank==0) && (rank < rest_eucli))\n",
    "          ++last;\n",
    "\n",
    "    for (i=1; i <= rank; ++i)\n",
    "    {\n",
    "      first += local_height;\n",
    "      last  += local_height;\n",
    "      if (rank < rest_eucli)\n",
    "          {\n",
    "              ++first;\n",
    "              ++last;\n",
    "          }\n",
    "    }\n",
    "\n",
    "    if (rank < rest_eucli) \n",
    "        ++local_height;\n",
    "\n",
    "    unsigned int num_elements = width*local_height;\n",
    "    if (rank == 0) printf(\"Using MPI\\n\");\n",
    "    total_devices = omp_get_num_devices();\n",
    "    printf(\"I am rank %2d and my range is [%5d, %5d[ ie %10d elements. Runing on %d GPUs.\\n\", rank, first, last, num_elements, total_devices);\n",
    "    unsigned char* restrict picture = (unsigned char*) malloc(num_elements*sizeof(unsigned char));\n",
    "#pragma omp target data map(tofrom:picture[0:num_elements]) device(rank)\n",
    "{    \n",
    "#pragma omp target teams distribute parallel for simd collapse(2) device(rank)\n",
    "    for (unsigned int i=0; i<local_height; ++i)\n",
    "        for (unsigned int j=0; j<width; ++j)\n",
    "        {\n",
    "            float complex c;\n",
    "            c = min_re + j*step_w * (max_re - min_re) + \\\n",
    "                I * (min_im +  ((i+first) * step_h) * (max_im - min_im));\n",
    "            picture[width*i+j] = (unsigned char) (255-rank*(255/nb_procs)) - mandelbrot_iterations(c);\n",
    "        }\n",
    "}\n",
    "    output(picture, first*width, num_elements); \n",
    "    MPI_Finalize();\n",
    "\n",
    "    // Measure time\n",
    "    clock_gettime(CLOCK_MONOTONIC_RAW, &end);\n",
    "    unsigned long int delta_us = (end.tv_sec - start.tv_sec) * 1000000 + (end.tv_nsec - start.tv_nsec) / 1000;\n",
    "    printf(\"The time to generate the mandelbrot picture was %lu us\\n\", delta_us);\n",
    "    return EXIT_SUCCESS;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38856112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idrcomp import show_gray\n",
    "show_gray(\"mandel.gray\", 1000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77465ac",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Using NV-link with OpenMP target\n",
    "\n",
    "You can specify to the accelerator the pointer to a given data structure already present on the device that should be used with `use_device_addr` clause of the `data` construct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3ab46",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "As an exercise, you can complete the following MPI code that measures the bandwidth between the GPUs:\n",
    "\n",
    "1. Add directives to create the buffers on the GPU\n",
    "2. Measure the effective bandwidth between GPUs by adding the directives necessary to transfer data from one GPU to another one in the following cases:\n",
    "\n",
    "- Not using NVLink\n",
    "- Using NVLink\n",
    "\n",
    "We have a bug for MPI in the notebooks and you need to save the file before running the next cell.\n",
    "It is a good way to pratice manual building!\n",
    "Please add the correct extension for the language you are running.\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_MultiGPU_mpi_exercise.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun  --cliopts \"2000 1000\" -m 4 -g 4 --options \"-mp=gpu -gpu=cc70 -Minfo=all\" \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <openacc.h>\n",
    "#include <math.h>\n",
    "#include \"../examples/C/init_openacc.h\"\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    initialisation_openacc();\n",
    "    MPI_Init(&argc, &argv);\n",
    "    fflush(stdout);\n",
    "    double start;\n",
    "    double end;\n",
    "    \n",
    "    int size = 2e8/8;\n",
    "    \n",
    "    double* send_buffer = (double*)malloc(size*sizeof(double));\n",
    "    double* receive_buffer = (double*)malloc(size*sizeof(double));\n",
    "    // MPI Stuff\n",
    "    int my_rank;\n",
    "    int comm_size;\n",
    "    int reps = 5;\n",
    "    double data_volume = (double)reps*(double)size*sizeof(double)*pow(1024,-3.0);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "    MPI_Status status;\n",
    "    \n",
    "    // OpenACC Stuff\n",
    "    acc_device_t device_type = acc_get_device_type();\n",
    "    int num_gpus = acc_get_num_devices(device_type);\n",
    "    int my_gpu = my_rank%num_gpus;\n",
    "    acc_set_device_num(my_gpu, device_type); \n",
    "    for (int i = 0; i<comm_size; ++i)\n",
    "    {\n",
    "        for (int j=0; j < comm_size; ++j)\n",
    "        {\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                start = MPI_Wtime();\n",
    "                for (int k = 0 ; k < reps; ++k)\n",
    "                    MPI_Ssend(send_buffer, size, MPI_DOUBLE, j, 0, MPI_COMM_WORLD);\n",
    "            }\n",
    "            if (my_rank == j && i != j)\n",
    "            {\n",
    "                for (int k = 0 ; k < reps; ++k)\n",
    "                    MPI_Recv(receive_buffer, size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                end = MPI_Wtime();\n",
    "                printf(\"bandwidth %d->%d: %10.5f GB/s\\n\", i, j, data_volume/(end-start));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bb2a0",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### Solution\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_MultiGPU_mpi_solution.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4130ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%idrrun  --cliopts \"2000 1000\" -m 4 -g 4 --options \"-mp=gpu -gpu=cc70 -Minfo=all\" \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <openmp.h>\n",
    "#include <math.h>\n",
    "#include \"../../examples/init_omp_target.h\"\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    initialisation_openacc();\n",
    "    MPI_Init(&argc, &argv);\n",
    "    fflush(stdout);\n",
    "    double start;\n",
    "    double end;\n",
    "    \n",
    "    int size = 200000000/8;\n",
    "    \n",
    "    double* send_buffer = (double*)malloc(size*sizeof(double));\n",
    "    double* receive_buffer = (double*)malloc(size*sizeof(double));\n",
    "    #pragma omp targer enter data map(alloc: send_buffer[:size], receive_buffer[:size])\n",
    "    // MPI Stuff\n",
    "    int my_rank;\n",
    "    int comm_size;\n",
    "    int reps = 5;\n",
    "    double data_volume = (double)reps*(double)size*sizeof(double)*pow(1024,-3.0);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "    MPI_Status status;\n",
    "    \n",
    "    // OpenMP target Stuff\n",
    "    int num_gpus = omp_get_num_devices();\n",
    "    int my_gpu = my_rank%num_gpus;\n",
    "    acc_set_device_num(my_gpu, device_type); \n",
    "    for (int i = 0; i<comm_size; ++i)\n",
    "    {\n",
    "        for (int j=0; j < comm_size; ++j)\n",
    "        {\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                start = MPI_Wtime();\n",
    "                #pragma omp target data use_device_ptr(send_buffer)\n",
    "                {\n",
    "                    for (int k = 0 ; k < reps; ++k)\n",
    "                        MPI_Ssend(send_buffer, size, MPI_DOUBLE, j, 0, MPI_COMM_WORLD);\n",
    "                }\n",
    "            }\n",
    "            if (my_rank == j && i != j)\n",
    "            {\n",
    "                #pragma omp target data use_device_ptr(receive_buffer)\n",
    "                {\n",
    "                    for (int k = 0 ; k < reps; ++k)\n",
    "                        MPI_Recv(receive_buffer, size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n",
    "                }\n",
    "            }\n",
    "            if (my_rank == i && i != j)\n",
    "            {\n",
    "                end = MPI_Wtime();\n",
    "                printf(\"bandwidth %d->%d: %10.5f GB/s\\n\", i, j, data_volume/(end-start));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    #pragma omp targer exit data map(delete: send_buffer[:size], receive_buffer[:size])\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978be45d",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Asynchronism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecef8a0",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Concurrent executions within the same stream\n",
    "\n",
    "An implicit barrier is set at the end of each `target` construct to ensure that the parent task (the task on the host) can not move on until the target task has ended. You can disable this implicit behavior and submit several kernels on the GPU by explicitly adding the `nowait` clause to the target construct.\n",
    "\n",
    "In order to avoid race conditions that could arise from the lack of barrier between kernels, it is possible to specify a scheduling of the kernels based on a dependency mechanism. To do so, you should use the `depend` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432110e",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_async_async_exercise.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff437502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%idrrun --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "double* create_mat(int dim, int stream)\n",
    "{\n",
    "    double* mat = (double*) malloc(dim*dim*sizeof(double));\n",
    "    return mat;\n",
    "}\n",
    "\n",
    "void init_mat(double* mat, int dim, double diag, int stream)\n",
    "{\n",
    "    for (int i=0; i<dim; ++i)\n",
    "        for (int j=0; j<dim; ++j)\n",
    "        {\n",
    "            mat[i*dim+j] = 0.;\n",
    "        }\n",
    "    for (int i=0; i<dim; ++i)\n",
    "        mat[i*dim+i] = diag;\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    int dim = 5000;\n",
    "    \n",
    "    double* restrict A = create_mat(dim, 1);\n",
    "    double* restrict B = create_mat(dim, 2);\n",
    "    double* restrict C = create_mat(dim, 3);\n",
    "    \n",
    "    init_mat(A, dim, 6.0, 1);\n",
    "    init_mat(B, dim, 7.0, 2);\n",
    "    init_mat(C, dim, 0.0, 3);\n",
    "\n",
    "    for (int i=0; i<dim; ++i)\n",
    "        for (int k=0; k<dim; ++k)\n",
    "            for (int j=0; j<dim; ++j)\n",
    "            {\n",
    "                C[i*dim+j] += A[i*dim+k] * B[k*dim+j];\n",
    "            }\n",
    "    }\n",
    "    printf(\"Check that value is equal to 42.: %f\\n\", C[0]);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dc1f8",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Solution\n",
    "\n",
    "Example stored in: `../../examples/C/OpenMP_async_async_solution.c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0d949",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%idrrun --options \"-mp=gpu -gpu=cc70 -Minfo=all\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "double* create_mat(int dim, int stream)\n",
    "{\n",
    "    double* mat = (double*) malloc(dim*dim*sizeof(double));\n",
    "    #pragma omp enter data map(alloc:mat[0:dim*dim]) nowait depend(out:mat)\n",
    "    return mat;\n",
    "}\n",
    "\n",
    "void init_mat(double* mat, int dim, double diag, int stream)\n",
    "{\n",
    "    #pragma acc parallel loop present(mat[0:dim*dim]) async(stream)\n",
    "    #pragma omp teams distribute parallel for simd collapse(2) nowait depend(inout:mat)\n",
    "    for (int i=0; i<dim; ++i)\n",
    "        for (int j=0; j<dim; ++j)\n",
    "        {\n",
    "            mat[i*dim+j] = 0.;\n",
    "        }\n",
    "    #pragma omp teams distribute parallel for simd nowait depend(in:mat)\n",
    "    for (int i=0; i<dim; ++i)\n",
    "        mat[i*dim+i] = diag;\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    int dim = 5000;\n",
    "    \n",
    "    double* restrict A = create_mat(dim, 1);\n",
    "    double* restrict B = create_mat(dim, 2);\n",
    "    double* restrict C = create_mat(dim, 3);\n",
    "    \n",
    "    init_mat(A, dim, 6.0, 1);\n",
    "    init_mat(B, dim, 7.0, 2);\n",
    "    init_mat(C, dim, 0.0, 3);\n",
    "\n",
    "    #pragma omp target teams distribut parallel for simd collaspe(3)\t    \n",
    "    for (int i=0; i<dim; ++i)\n",
    "        for (int k=0; k<dim; ++k)\n",
    "            for (int j=0; j<dim; ++j)\n",
    "            {\n",
    "                C[i*dim+j] += A[i*dim+k] * B[k*dim+j];\n",
    "            }\n",
    "    }\n",
    "    #pragma omp target exit data map(delete:A,B)\n",
    "    #pragma omp target exit data map(from:C[:dim*dim])\n",
    "    printf(\"Check that value is equal to 42.: %f\\n\", C[0]);\n",
    "    return 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU Directives",
   "language": "python",
   "name": "gpu_directives"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
